import torch.utils.data as data
from abc import ABC
from torchvision import transforms
import torch
import numpy as np
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import pickle
import random
import copy
import pandas as pd
import torch.nn.functional as F
from data.utility import *
from data.data_augmentation import *
from data.public_pd_datareader import PDReader
from data.augmentations import MirrorReflection, RandomRotation, RandomNoise, axis_mask
from learning.utils import compute_class_weights
from collections import defaultdict


_TOTAL_SCORES = 3
_MAJOR_JOINTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
_ROOT = 0
_MIN_STD = 1e-4

METADATA_MAP = {'gender': 0, 'age': 1, 'height': 2, 'weight': 3, 'bmi': 4}
# PD_PATH_POSES = '/media/hdd/minlin/MotionEncoders_parkinsonism_benchmark/PD_3D_motion-capture_data/C3Dfiles_processed_new/'
PD_PATH_POSES = '/media/hdd/minlin/MotionEncoders_parkinsonism_benchmark/PD_3D_motion-capture_data/C3Dfiles_cleaned_sequences/'
PD_PATH_SENSORS = '/media/hdd/minlin/MotionEncoders_parkinsonism_benchmark/PD_3D_motion-capture_data/GRF_processed/'
PD_PATH_LABELS = '/media/hdd/minlin/MotionEncoders_parkinsonism_benchmark/PD_3D_motion-capture_data/PDGinfo.xlsx'



class ProcessedDataset(data.Dataset):
    """
        Loads the preprocessed .pkl files generated by DataPreprocessor.
        Implements __getitem__() and __len__() for PyTorch DataLoader compatibility.
        Returns processed gait skeleton sequences, labels, and metadata to the training pipeline.
        
        pose_dict: ('SUB20_on_walk_19_2', array([[[ 0. , 0. , 0. ],..]..])
        sensor_dict: ('SUB20_on_left', array([[[ 0. , 0. , 0. ],..]..])
        pose/sensor_label_dict: ('SUB20_on', 2) 0/1/2 updrs score
        metadata_dict: ('SUB20_on_walk_19_2', array([[0, 0.8108, ]], dtype=float32))
        video_names[]: SUB20_on_walk_19_2 
        selected_subjects[]: ['SUB01', 'SUB02', 'SUB03']
    """
    def __init__(self, pose_dict=None, pose_label_dict=None, sensor_dict=None, sensor_length=None, sensor_label_dict=None, metadata_dict=None, video_names=None, selected_subjects=None, label_dict=None, params=None, transform=None, modality=None, synchronized: bool = False, seed: int = 0):
        super().__init__()
        self.pose_dict          = pose_dict
        self.sensor_dict        = sensor_dict
        self.turn_sensor_seg_length   = sensor_length
        self.selected_subjects  = selected_subjects # List of subject IDs to keep
        self.label_dict         = label_dict
        self.modality           = modality
        self._params            = params if params else {}
        self.transform          = transform
        self.mode               = 'eval' if len(selected_subjects) == 3 else 'train'
        self.dataset            = 'walk' if label_dict is None else 'turn'
        self.num_classes        = 3
        self._params['data_centered']   = True    # /False
        self._params['data_norm']       = 'minmax'    # or 'zscore' or None
        if self._params.get('data_centered', False):
            self.center_skeleton_poses()
        if self._params.get('data_norm', None):
            self.normalize_skeleton_poses()
            
        if pose_label_dict is not None:  # the 'walk' dataset
            self.pose_label_dict    = pose_label_dict 
            self.sensor_label_dict  = sensor_label_dict
            self.metadata_dict      = metadata_dict
            self.video_names        = video_names
            self.pose_label_dict    = {vn: self.pose_label_dict[vn] for vn in self.pose_label_dict.keys() if vn.split("_")[0] in self.selected_subjects}
            self.sensor_label_dict  = {sn: self.sensor_label_dict[sn] for sn in self.sensor_label_dict.keys() if sn.split("_")[0] in self.selected_subjects}
        self.poses  = {vn: self.pose_dict[vn] for vn in self.pose_dict.keys() if vn.split("_")[0] in self.selected_subjects}
        self.sensor = {sn: self.sensor_dict[sn] for sn in self.sensor_dict.keys() if sn.split("_")[0] in self.selected_subjects}
        
        self.sub_pose_mapping = self.form_sub_posekey_mapping()
        self.sub_sensor_mapping = self.form_sub_sensorkey_mapping()
        
        # build time‐aligned pairs if requested:
        self.synchronized = synchronized
        if self.synchronized:
            self.synced_pairs = []
            for sub in self.selected_subjects:
                pkeys = self.sub_pose_mapping[sub]
                skeys = self.sub_sensor_mapping[sub]
                seg2s = defaultdict(list)
                for sk in skeys:
                    segment = "_".join(sk.split("_")[-2:])
                    seg2s[segment].append(sk)
                for pk in pkeys:
                    segment = "_".join(pk.split("_")[-2:])
                    for sk in seg2s.get(segment, []):
                        self.synced_pairs.append((pk, sk))
        
        if self.synchronized and self.mode=='eval':
            random.seed(seed)
            # group synced pairs by true class
            cls2pairs = defaultdict(list)
            for pk, sk in self.synced_pairs:
                label = self._get_label(pk)   # same for pk & sk
                cls2pairs[label].append((pk, sk))
            # find maximum
            max_n = max(len(v) for v in cls2pairs.values())
            # oversample each class to match max_n
            balanced = []
            for c, pairs in cls2pairs.items():
                for _ in range(max_n):
                    balanced.append(random.choice(pairs))
            random.shuffle(balanced)
            self.synced_pairs = balanced

        
        if len(self.selected_subjects) == 3:    # evaluation mode
            self.mode = 'eval'
            # self.select_eval_trial()
            print(f"Evaluation skeletons samples: {len(self.poses)} sensors smaples: {len(self.sensor)}")
    
    def __len__(self):
        """
            use max number of samples for each modality --- evaluation-time oversampling, reports accuracy
        """
        if self.synchronized:
            return len(self.synced_pairs)
        else:
            sample_count = 0
            if self.modality == 'sensor':
                if self.dataset == 'turn':
                    if self.mode == 'train':
                        sample_count = len(self.sensor)
                    elif self.mode == 'eval':
                        sample_count = self.find_max_len() * self.num_classes
                elif self.dataset == 'walk':
                    sample_count = sum(each.shape[1] for each in self.sensor.values())
                    
            elif self.modality == 'skeleton':
                if self.dataset == 'turn' and self.mode == 'eval': # eval mode for 'turn' dataset
                    sample_count = self.find_max_len() * self.num_classes
                else:
                    sample_count = len(self.poses)
                    
            elif self.modality == 'multimodal':
                # sample_count = max(len(self.poses), sum(each.shape[1] for each in self.sensor.values()))
                if self.dataset == 'turn' and self.mode == 'eval': # eval mode for 'turn' dataset
                    # 1. find largest number of samples of one class across two datasets
                    # e.g., the max len is in the sensor dataset where subject 1 has 3 trials hence (3 * seg_counts) samples 
                    # 2. max_len * number of classes --> make sure we can iterate through all sampels of that class \
                        # since we randomly pick the class (1/3 chance to select the max one) during sampling for evaluation (evaluation-time oversampling)
                    sample_count = self.find_max_len() * self.num_classes 
                else:
                    sample_count = max(len(self.poses), len(self.sensor))
            return sample_count

    def __getitem__(self, idx):
        if self.synchronized:
            # return only time‐aligned samples
            pk, sk = self.synced_pairs[idx]
            # direct‐load without randomization
            pose, lbl_p = self._retrieve_pose_by_key(pk)
            sensor, lbl_s = self._retrieve_sensor_by_key(sk)
            return self.form_sample(pose, lbl_p, sensor, lbl_s)
        else:
            sample = {}
            if self.modality == 'multimodal':
                pose, label_skeleton = self.retrieve_pose(idx)
                sensor, label_sensor = self.retrieve_sensor(idx)
                sample = self.form_sample(pose, label_skeleton, sensor, label_sensor)
            elif self.modality == 'skeleton':
                data, label = self.retrieve_pose(idx)
                sample = self.form_sample(data, label)
            elif self.modality == 'sensor':
                data, label = self.retrieve_sensor(idx)
                sample = self.form_sample(data, label)
            return sample

    # ------------------------------------------------------- Helper functions -------------------------------------------------------
    def form_sub_posekey_mapping(self):
        """
        form a mapping of subject to all pose keys: {'SUB01': ['SUB01_1_1', 'SUB01_1_2'], ...}
        """
        # 1) Create a dictionary to hold the mapping
        sub_pose_mapping = defaultdict(list)
        # 2) Iterate through the keys in self.poses
        for key in self.poses.keys():
            # 3) Extract the subject ID from the key
            subject_id = key.split("_")[0]
            # 4) Append the key to the list for that subject
            sub_pose_mapping[subject_id].append(key)
        # 5) Return the mapping
        return sub_pose_mapping
    
    def form_sub_sensorkey_mapping(self):
        """
        form a mapping of subject to all sensor keys
        """
        # 1) Create a dictionary to hold the mapping
        sub_sensor_mapping = defaultdict(list)
        # 2) Iterate through the keys in self.sensor
        for key in self.sensor.keys():
            # 3) Extract the subject ID from the key
            subject_id = key.split("_")[0]
            # 4) Append the key to the list for that subject
            sub_sensor_mapping[subject_id].append(key)
        # 5) Return the mapping
        return sub_sensor_mapping
            
    def select_eval_trial(self):
        subject_pose_trials = defaultdict(set)
        for key in self.poses:
            subj, trial = key.split("_")[:2]
            subject_pose_trials[subj].add(trial)

        subject_sensor_trials = defaultdict(set)
        for key in self.sensor:
            subj, trial = key.split("_")[:2]
            subject_sensor_trials[subj].add(trial)
            
        for sub in self.selected_subjects:
            pose_trials   = subject_pose_trials.get(sub, set())
            sensor_trials = subject_sensor_trials.get(sub, set())
            common_trials = list(pose_trials & sensor_trials)

            if not common_trials:
                # fallback if no exact overlap
                common_trials = list(pose_trials or sensor_trials)

            selected_trial = random.choice(common_trials)
            selected_name  = f"{sub}_{selected_trial}"

            # 3) Prune all other pose segments
            to_remove = [k for k in self.poses if k.startswith(f"{sub}_") and selected_name not in k]
            for k in to_remove:
                self.poses.pop(k)

            # 4) Prune all other sensor segments
            to_remove = [k for k in self.sensor if k.startswith(f"{sub}_") and selected_name not in k]
            for k in to_remove:
                self.sensor.pop(k)
            
    def center_skeleton_poses(self):
        """
        Subtracts the root joint from every joint so skeleton is around origin.
        """
        _ROOT = 0
        for key in self.pose_dict.keys():
            joints3d = self.pose_dict[key]
            if len(joints3d.shape) == 0:
                continue
            self.pose_dict[key] = joints3d - joints3d[:, _ROOT:_ROOT+1, :]
    
    def normalize_skeleton_poses(self):
        """
        Normalizes skeleton data according to self._params['data_norm'] (e.g. minmax, zscore).
        """
        data_norm = self._params['data_norm']
        if data_norm == 'minmax':
            normalized_pose_dict = {}
            for video_name in self.pose_dict:
                poses = self.pose_dict[video_name].copy()
                mins = np.min(np.min(poses, axis=0), axis=0)
                maxes = np.max(np.max(poses, axis=0), axis=0)
                poses = (poses - mins) / (maxes - mins + 1e-6)
                normalized_pose_dict[video_name] = poses
            self.pose_dict = normalized_pose_dict

        elif data_norm == 'rescaling':
            normalized_pose_dict = {}
            for video_name in self.pose_dict:
                poses = self.pose_dict[video_name].copy()
                mins = np.min(poses, axis=(0, 1))
                maxes = np.max(poses, axis=(0, 1))
                # scale into [-1, +1]
                poses = 2*(poses - mins)/(maxes - mins + 1e-6) - 1
                normalized_pose_dict[video_name] = poses
            self.pose_dict = normalized_pose_dict

        elif data_norm == 'zscore':
            # compute global mean & std
            all_data = []
            for k in self.pose_dict.keys():
                all_data.append(self.pose_dict[k])
            all_data = np.vstack(all_data)  # stack frames
            mean = np.mean(all_data, axis=0)  # shape (n_joints,3)
            std = np.std(all_data, axis=0)
            std[std < _MIN_STD] = 1

            # apply
            pose_dict_norm = {}
            for k in self.pose_dict.keys():
                tmp_data = self.pose_dict[k].copy()
                tmp_data = tmp_data - mean
                tmp_data = tmp_data / std
                pose_dict_norm[k] = tmp_data
            self.pose_dict = pose_dict_norm
    
    def retrieve_sensor(self, idx):
        if self.label_dict is None: # 'walk' dataset
            valid_rows = 65
            sample_count = sum(each.shape[1] for each in self.sensor.values())
            if idx >= sample_count:
                idx = np.random.randint(0, sample_count)
            for sensor_name, sensor_data in self.sensor.items():
                if idx > sensor_data.shape[1]-1:
                    idx -= sensor_data.shape[1]
                    continue
                else:
                    sensor = sensor_data[:valid_rows, idx]
                    label = self.sensor_label_dict[sensor_name]
        else:   # 'turn' dataset
            if self.mode == 'train':
                sample_count = len(self.sensor)
                if idx >= sample_count:
                    idx = np.random.randint(0, sample_count)
                sensor_name, sensor = list(self.sensor.items())[idx]
                sensor_name         = sensor_name.split("_")[0]
                label               = self.label_dict[sensor_name][0]
            elif self.mode == 'eval':
                # 1) Pick subject (round‐robin)
                selected_sub    = self.selected_subjects[idx % len(self.selected_subjects)]
                # 2) Pick random trial_segment from this subject
                selected_keys   = self.sub_sensor_mapping[selected_sub]
                if selected_keys:
                    sensor_name = random.choice(selected_keys)
                    sensor      = self.sensor[sensor_name]
                    label       = self.label_dict[sensor_name.split("_")[0]][0]
                
            if sensor.shape[0] >= self.turn_sensor_seg_length:
                sensor = sensor[:self.turn_sensor_seg_length]
            else:
                pad_len = self.turn_sensor_seg_length - sensor.shape[0]
                pad = np.zeros((pad_len,) + sensor.shape[1:], dtype=sensor.dtype)
                sensor = np.concatenate([sensor, pad], axis=0)
        sensor = np.asarray(sensor, dtype=np.float32)
        return sensor, label
    
    def retrieve_pose(self, idx):
        if self.mode == 'train':
            sample_count = len(self.poses)
            if idx < sample_count:
                pose_name, pose = list(self.poses.items())[idx]
            else:
                idx = np.random.randint(0, sample_count)
                pose_name, pose = list(self.poses.items())[idx]    
        elif self.mode == 'eval': 
            # 1) Pick subject (round‐robin)
            selected_sub    = self.selected_subjects[idx % len(self.selected_subjects)]
            # 2) Pick random trial_segment from this subject
            selected_keys   = self.sub_pose_mapping[selected_sub]
            if selected_keys:
                pose_name   = random.choice(selected_keys)
                pose        = self.poses[pose_name]
            else:
                print(f"Warning: No matching key found for subject {selected_sub}.")

        # 3) Look up the label
        if self.label_dict is not None: # agjusted for namings of the 'walk' dataset
            pose_name = pose_name.split("_")[0]
            label = self.label_dict[pose_name][0]
        else:
            pose_name = pose_name.split("_")[0] + "_" + pose_name.split("_")[1]
            label = self.pose_label_dict[pose_name]
        
        # 4) Trim or pad to exactly 101 frames
        if pose.shape[0] > 101:
            pose = pose[:101]
        elif pose.shape[0] < 101:
            pad_len = 101 - pose.shape[0]
            pad     = np.zeros((pad_len,) + pose.shape[1:], dtype=pose.dtype)
            pose    = np.concatenate([pose, pad], axis=0)     
        return pose.astype(np.float32), label
    
    def form_sample(self, data, label, data1=None, label1=None):
        if data1 is not None:
            return {
                "skeleton": torch.tensor(data),  # shape: (101, skeleton_dim) or (101, 7, 3)
                "sensor": torch.tensor(data1),   # shape: (65, 3)  /  (426, 6)
                "label_skeleton": torch.tensor(label),
                "label_sensor": torch.tensor(label1)
            }
        elif data.shape[0] == 101:
            return {
                "skeleton": torch.tensor(data),  # shape: (101, skeleton_dim)
                "label_skeleton": torch.tensor(label),
            }
        elif data.shape[0] == self.turn_sensor_seg_length: # 65
            return {
                "sensor": torch.tensor(data),  # shape: (65, 3)
                "label_sensor": torch.tensor(label),
            }
        else:
            raise ValueError(f"Data shape {data.shape} is not valid. Expected (101, skeleton_dim) or (65, 3).")
    
    def find_max_len(self):
        max_len = 0
        # 1) Find maximum number of samples for each subject
        if self.modality == 'multimodal':
            for each in self.selected_subjects:
                max_len = max(max_len, len(self.sub_pose_mapping[each]))
                max_len = max(max_len, len(self.sub_sensor_mapping[each]))
        elif self.modality == 'skeleton':
            for each in self.selected_subjects:
                max_len = max(max_len, len(self.sub_pose_mapping[each]))
        elif self.modality == 'sensor': 
            for each in self.selected_subjects:
                max_len = max(max_len, len(self.sub_sensor_mapping[each]))   
        return max_len
        
    def _get_label(self, key: str) -> int:
        """
        Lookup the integer label for the given clip key.
        - For 'turn', labels_dict maps subject_id -> [label]
        - For 'walk', pose_label_dict / sensor_label_dict map full key -> label
        """
        if self.dataset == 'turn':
            subj = key.split("_")[0]
            return self.label_dict[subj][0]
        else:  # 'walk'
            # keys are full video names, and you have two dicts:
            if key in self.pose_label_dict:
                return self.pose_label_dict[key]
            else:
                return self.sensor_label_dict[key]

    def _get_joint_orders(self):
        joints = _MAJOR_JOINTS
        return joints

    def _retrieve_pose_by_key(self, key):
        """
        Deterministically load exactly the given pose key (no random sampling).
        Returns (pose_array, label).
        """
        # 1) look up raw pose
        pose = self.pose_dict[key]

        # 2) pad or trim to 101 frames
        if pose.shape[0] > 101:
            pose = pose[:101]
        elif pose.shape[0] < 101:
            pad_len = 101 - pose.shape[0]
            pad = np.zeros((pad_len,) + pose.shape[1:], dtype=pose.dtype)
            pose = np.concatenate([pose, pad], axis=0)

        # 3) get label
        if self.dataset == 'turn':
            # label_dict maps subject -> [label]
            subj = key.split("_")[0]
            label = self.label_dict[subj][0]
        else:
            # walk: pose_label_dict maps "SUBXX_on" -> label
            parts = key.split("_")
            video_name = f"{parts[0]}_{parts[1]}"
            label = self.pose_label_dict[video_name]

        return pose.astype(np.float32), label

    def _retrieve_sensor_by_key(self, key):
        """
        Deterministically load exactly the given sensor key (no random sampling).
        Returns (sensor_array, label).
        """
        # 1) look up raw sensor
        sensor = self.sensor_dict[key]

        # 2) pad or trim to turn_sensor_seg_length
        L = sensor.shape[0]
        T = self.turn_sensor_seg_length
        if L >= T:
            sensor = sensor[:T]
        else:
            pad_len = T - L
            pad = np.zeros((pad_len,) + sensor.shape[1:], dtype=sensor.dtype)
            sensor = np.concatenate([sensor, pad], axis=0)

        # 3) get label
        if self.dataset == 'turn':
            subj = key.split("_")[0]
            label = self.label_dict[subj][0]
        else:
            # walk: sensor_label_dict maps full key -> label
            label = self.sensor_label_dict[key]

        return sensor.astype(np.float32), label


def main():
    params = {
        'merge_last_dim': True,
        "skeleton_input_dim": 51,   # 17 joints * 3 dims
        "skeleton_feat_dim": 32,
        "sensor_feat_dim": 32,
        "backbone_dim": 64,
        "num_classes_skel": 3,
        "num_classes_sensor": 3,
        "learning_rate": 1e-3,
        "epochs": 3,
        "batch_size": 8,
    }
    
    # reader = DummyPDReader(num_samples=100, num_frames=101, num_joints=17)
    reader = PDReader(PD_PATH_POSES, PD_PATH_SENSORS, PD_PATH_LABELS)
    
    dataset = ProcessedDataset(
        sensor_dict=reader.sensor_dict,
        pose_dict=reader.pose_dict,
        labels_dict=reader.labels_dict,
        metadata_dict=reader.metadata_dict,
        video_names=reader.video_names,
        participant_ID=reader.participant_ID,
        params=params,
        fold=1,
        transform=None
    )
    
    # # With __getitem__ already padding sensor data, we can use the default collate.
    from torch.utils.data import DataLoader
    dataloader = DataLoader(dataset, batch_size=params["batch_size"], shuffle=True)
    
    # Test one batch
    for batch in dataloader:
        print("Batch keys:", batch.keys())
        print("Skeleton batch shape:", batch["skeleton"].shape)
        print("Sensor left batch shape:", batch["sensor_left"].shape)
        print("Sensor right batch shape:", batch["sensor_right"].shape)
        print("Labels shape:", batch["label"].shape)
        break

# if __name__ == "__main__":
#     main()

class DataPreprocessor(ABC):
    """
    Loads raw gait pose data, labels, metadata, and video names.
    Implements pose normalization and centering
    Cross-Validation Splits: Leave-One-Out (LOO)
    """
    def __init__(self, raw_data, params=None):
        self.pose_dict = raw_data.pose_dict
        self.labels_dict = raw_data.labels_dict
        self.metadata_dict = raw_data.metadata_dict
        self.video_names = raw_data.video_names
        self.participant_ID = raw_data.participant_ID
        self.params = params

        self.data_dir = self.params['data_path']

    def __len__(self):
        return len(self.labels_dict)

    def center_poses(self):
        """
        Implementation:
            1. Iterates over each video in self.pose_dict.
            2. Finds the root joint’s 3D coordinates ( _ROOT ) for each frame.
            3. Subtracts it from every joint so that the skeleton is “centered” around the root.
            
            Why:
                Eliminates absolute position offset in the lab coordinate system, focusing on relative movement of joints.
        """
        for key in self.pose_dict.keys():
            joints3d = self.pose_dict[key]  # (n_frames, n_joints, 3)
            self.pose_dict[key] = joints3d - joints3d[:, _ROOT:_ROOT + 1, :]

    def normalize_poses(self):
        if self.params['data_norm'] == 'minmax':
            """
            Normalize each pose along each axis by video. Divide by the largest value in each direction
            and center around the origin.
            :param pose_dict: dictionary of poses
            :return: dictionary of normalized poses
            """
            normalized_pose_dict = {}
            for video_name in self.pose_dict:
                poses = self.pose_dict[video_name].copy()

                mins = np.min(np.min(poses, axis=0), axis=0)
                maxes = np.max(np.max(poses, axis=0), axis=0)

                poses = (poses - mins) / (maxes - mins)

                normalized_pose_dict[video_name] = poses
            self.pose_dict = normalized_pose_dict

        elif self.params['data_norm'] == 'rescaling':
            normalized_pose_dict = {}
            for video_name in self.pose_dict:
                poses = self.pose_dict[video_name].copy()

                mins = np.min(poses, axis=(0, 1))
                maxes = np.max(poses, axis=(0, 1))

                poses = (2 * (poses - mins) / (maxes - mins)) - 1

                normalized_pose_dict[video_name] = poses
            self.pose_dict = normalized_pose_dict

        elif self.params['data_norm'] == 'zscore':
            norm_stats = self.compute_norm_stats()
            pose_dict_norm = self.pose_dict.copy()
            for k in self.pose_dict.keys():
                tmp_data = self.pose_dict[k].copy()
                tmp_data = tmp_data - norm_stats['mean']
                tmp_data = np.divide(tmp_data, norm_stats['std'])
                pose_dict_norm[k] = tmp_data
            self.pose_dict = pose_dict_norm

    def compute_norm_stats(self):
        """
            Calculates the global mean and std across all frames in self.pose_dict.
        """
        all_data = []
        for k in self.pose_dict.keys():
            all_data.append(self.pose_dict[k])
        all_data = np.vstack(all_data)
        print('[INFO] ({}) Computing normalization stats!')
        norm_stats = {}
        mean = np.mean(all_data, axis=0)
        std = np.std(all_data, axis=0)
        std[np.where(std < _MIN_STD)] = 1

        norm_stats['mean'] = mean  # .ravel()
        norm_stats['std'] = std  # .ravel()
        return norm_stats
        
    def generate_leave_one_out_folds(self, clip_dict, save_dir, labels_dict):
        """
        Generate folds for leave-one-out CV.
        :param clip_dict: dictionary of clips for each video
        :param save_dir: save directory for folds
        """
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        video_names_list = list(clip_dict.keys())
        fold = 0

        dataset_name = self.params['dataset']
        val_folds_name = 'val_PD_SUBs_folds.pkl' if dataset_name == 'PD' else 'val_AMBIDs_folds.pkl'
        val_folds_path = os.path.join(save_dir, '..', val_folds_name)

        val_folds_exists = os.path.exists(val_folds_path)

        if not val_folds_exists:
            val_subs_folds = []
            print(f'[INFO] Previous selected {dataset_name} validation set does not exist.')
        else:
            val_subs_folds = pickle.load(open(val_folds_path, "rb"))


        for j in range(len(self.participant_ID)):
            train_list, val_list, test_list = [], [], []

            participant_ID_cloned = copy.deepcopy(self.participant_ID)
            subject_id = participant_ID_cloned.pop(j)

            class_participants = {}
            for participant in participant_ID_cloned:
                participant_labels = [labels_dict[key] for key in labels_dict if key.startswith(participant + "_on") or key.startswith(participant + "_off")]
                if participant_labels:
                    class_participants[participant] = participant_labels[0]  # Use the first label as the class

            if not val_folds_exists:
                val_subs = []
                for class_id in range(3):  # Assuming classes are 0, 1, 2
                    class_participants_for_class = [participant for participant, class_label in class_participants.items() if class_label == class_id]
                    for _ in range(2):  # Select 2 participants from each class
                        val_idx = random.randint(0, len(class_participants_for_class) - 1)
                        val_subs.append(class_participants_for_class.pop(val_idx))
                val_subs_folds.append(val_subs)
            else:
                val_subs = val_subs_folds[j]

            for k in range(len(video_names_list)):
                video_name = video_names_list[k]
                # augmented = any(augmentation in video_name for augmentation in self.params['augmentation'])
                if dataset_name == 'PD':
                    if subject_id == video_name.split("_")[0]:
                        # if not augmented:
                        test_list.append(video_name)
                    elif video_name.split("_")[0] in val_subs:
                        val_list.append(video_name)
                    else:
                        train_list.append(video_name)
            print("Fold: ", fold)
            fold += 1
            train, validation, test = self.generate_pose_label_videoname(clip_dict, train_list, val_list, test_list)
            pickle.dump(train_list, open(os.path.join(save_dir, f"{dataset_name}_train_list_{fold}.pkl"), "wb"))
            pickle.dump(test_list, open(os.path.join(save_dir, f"{dataset_name}_test_list_{fold}.pkl"), "wb"))
            pickle.dump(val_list, open(os.path.join(save_dir, f"{dataset_name}_validation_list_{fold}.pkl"), "wb"))
            pickle.dump(train, open(os.path.join(save_dir, f"{dataset_name}_train_{fold}.pkl"), "wb"))
            pickle.dump(test, open(os.path.join(save_dir, f"{dataset_name}_test_{fold}.pkl"), "wb"))
            pickle.dump(validation, open(os.path.join(save_dir, f"{dataset_name}_validation_{fold}.pkl"), "wb"))
        pickle.dump(self.labels_dict, open(os.path.join(save_dir, f"{dataset_name}_labels.pkl"), "wb"))

        if not val_folds_exists:
            pickle.dump(val_subs_folds, open(val_folds_path, "wb"))

    def get_data_split(self, split_list, clip_dict):
        split = {'pose': [], 'label': [], 'video_name': [], 'metadata': []}
        for video_name in split_list:
            clips = clip_dict[video_name]
            for clip in clips:
                split['label'].append(self.labels_dict[video_name])
                split['pose'].append(clip)
                split['video_name'].append(video_name)
                split['metadata'].append(self.metadata_dict[video_name])
        return split

    def generate_pose_label_videoname(self, clip_dict, train_list, val_list, test_list):
        train = self.get_data_split(train_list, clip_dict)
        val = self.get_data_split(val_list, clip_dict)
        test = self.get_data_split(test_list, clip_dict)

        #print how many samples are in each split
        print(f"Train Length: {len(train['video_name'])}")
        print(f"Validation Length: {len(val['video_name'])}")
        print(f"Test Length: {len(test['video_name'])}")
        return train, val, test

    @staticmethod
    def resample(original_length, target_length):
        """
        Adapted from https://github.com/Walter0807/MotionBERT/blob/main/lib/utils/utils_data.py#L68

        Returns an array that has indices of frames. elements of array are in range (0, original_length -1) and
        we have target_len numbers (So it interpolates the frames)
        """
        even = np.linspace(0, original_length, num=target_length, endpoint=False)
        result = np.floor(even)
        result = np.clip(result, a_min=0, a_max=original_length - 1).astype(np.uint32)
        return result
